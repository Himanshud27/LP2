from fpdf import FPDF

# Extracted HTML snippet manually (simulated here, can be dynamically pulled in full deployment)
html_snippet = """




1. Which algorithm is commonly used for optimizing the weights of a neural network?A. Gradient DescentB. Random SearchC. Genetic AlgorithmD. Simulated AnnealingAnswer: AExplanation: Gradient Descent iteratively updates model parameters in the direction of the negative gradient of the loss to minimize the cost function. citeturn0search0
2. What does the term “deep” in deep learning refer to?A. Use of large datasetsB. Use of multiple layers in neural networksC. Use of high-dimensional dataD. Use of deep sea-inspired algorithmsAnswer: BExplanation: “Deep” denotes neural networks with many hidden layers, enabling hierarchical feature learning. citeturn0search10
3. Which activation function helps mitigate the vanishing gradient problem?A. SigmoidB. TanhC. ReLUD. SoftmaxAnswer: CExplanation: ReLU sets negative inputs to zero while keeping positive inputs unchanged, preserving gradient flow better than sigmoid or tanh. citeturn0search10
4. What is the main purpose of a loss function?A. To initialize weightsB. To measure prediction errorC. To activate neuronsD. To adjust learning rateAnswer: BExplanation: The loss function quantifies the discrepancy between model predictions and true labels, guiding optimization. citeturn0search0
5. What is “dropout” used for in training neural networks?A. Reducing model sizeB. Preventing overfitting by randomly deactivating neuronsC. Speeding up training by dropping dataD. Increasing the number of parametersAnswer: BExplanation: Dropout randomly disables units during training to prevent co‑adaptation and improve generalization. citeturn0search1
6. Batch normalization is primarily used to:A. Normalize input data onlyB. Normalize activations within a layer for faster and more stable trainingC. Reduce dataset sizeD. Increase network depthAnswer: BExplanation: It standardizes layer inputs by correcting mean and variance, mitigating internal covariate shift. citeturn0search2
7. Which optimizer adapts the learning rate for each parameter?A. SGDB. RMSPropC. AdamD. MomentumAnswer: CExplanation: Adam tracks first and second moments of gradients, automatically tuning per‑parameter learning rates. citeturn0search3
8. “Early stopping” is a technique to:A. Stop training when validation loss stops improvingB. Stop training after a fixed number of epochsC. Pause training for tuningD. Stop data preprocessingAnswer: AExplanation: Training halts once performance on a held‑out set ceases to improve, preventing overfitting. citeturn0search10
9. Which of the following is a form of regularization?A. Data augmentationB. Batch normalizationC. DropoutD. All of the aboveAnswer: DExplanation: Each method helps reduce overfitting: augmenting data, normalizing activations, or dropping units. citeturn0search1turn0search2
10. The term “epoch” refers to:A. One forward passB. One backward passC. One complete pass through the training datasetD. One layer updateAnswer: CExplanation: An epoch is when the model has seen every sample in the training set once. citeturn0search10
11. A convolutional neural network (CNN) is most commonly used for:A. Time series dataB. Image dataC. Tabular dataD. Text dataAnswer: BExplanation: CNNs excel at processing grid‑structured inputs like images via convolutional filters. citeturn0search6
12. In CNNs, pooling layers are used to:A. Increase feature map sizeB. Downsample feature mapsC. Normalize activationsD. Add non-linearityAnswer: BExplanation: Pooling reduces spatial dimensions, summarizing regions to lower computation and overfitting. citeturn0search6
13. Which pooling operation takes the average of values in a region?A. Max poolingB. Average poolingC. Sum poolingD. L2 poolingAnswer: BExplanation: Average pooling outputs the mean activation over each receptive field. citeturn0search6
14. Recurrent Neural Networks (RNNs) are primarily used for:A. Image classificationB. Sequential data processingC. ClusteringD. Dimensionality reductionAnswer: BExplanation: RNNs process sequences by maintaining internal state across time steps. citeturn0search5
15. The key limitation of basic RNNs is:A. OverfittingB. UnderfittingC. Vanishing/exploding gradientsD. Slow inferenceAnswer: CExplanation: Long‑term dependencies suffer due to gradients that shrink or explode over many steps. citeturn0search5
16. Long Short-Term Memory (LSTM) networks address vanishing gradients by using:A. Convolution operationsB. Gated cellsC. DropoutD. Skip connectionsAnswer: BExplanation: LSTMs use input, forget, and output gates to regulate gradient flow over time. citeturn0search5
17. The softmax function is typically used in:A. Hidden layersB. Output layer for multi-class classificationC. Weight initializationD. Data normalizationAnswer: BExplanation: Softmax converts logits to class probabilities summing to one. citeturn0search10
18. Which technique is used to prevent neurons from co-adapting too much?A. Batch normalizationB. DropoutC. Gradient clippingD. Weight decayAnswer: BExplanation: Randomly dropping units forces the network to learn redundant representations. citeturn0search1
19. Gradient clipping is used to:A. Prevent vanishing gradientsB. Prevent exploding gradientsC. Speed up convergenceD. Regularize the modelAnswer: BExplanation: It imposes a maximum norm on gradients to avoid extremely large updates. citeturn0search10
20. Autoencoders are primarily used for:A. ClassificationB. Dimensionality reductionC. Reinforcement learningD. Object detectionAnswer: BExplanation: Autoencoders learn compact latent representations by reconstructing inputs. citeturn0search7
21. Variational Autoencoders (VAEs) introduce:A. Deterministic encodingsB. Probabilistic encodingsC. Convolutional encodingsD. Recurrent encodingsAnswer: BExplanation: VAEs model latent variables with distributions, enabling generative sampling. citeturn0search7
22. Which loss is commonly used in VAEs?A. MSEB. Cross-entropyC. KL divergenceD. Hinge lossAnswer: CExplanation: The KL divergence term regularizes the learned latent distribution toward the prior. citeturn0search7
23. Generative Adversarial Networks (GANs) consist of:A. Two autoencodersB. Generator and discriminatorC. Encoder and decoder onlyD. Multiple classifiersAnswer: BExplanation: The generator creates samples while the discriminator distinguishes real from fake. citeturn0search4
24. In a GAN, the discriminator’s task is to:A. Generate fake samplesB. Distinguish real from fake dataC. Encode dataD. Decode dataAnswer: BExplanation: It learns to classify inputs as genuine or synthesized. citeturn0search4
25. Which technique helps stabilize GAN training?A. DropoutB. Label smoothingC. Batch normalizationD. All of the aboveAnswer: DExplanation: Regularization, smoothing labels, and normalizing activations can all improve stability. citeturn0search1turn0search2
26. Residual connections (as in ResNet) help with:A. Vanishing gradientsB. OverfittingC. Reducing parametersD. Data augmentationAnswer: AExplanation: Skip connections provide alternate gradient pathways, mitigating vanishing gradients. citeturn0search8
27. The term “attention” in neural networks refers to:A. A loss functionB. A mechanism to weigh input features differentlyC. A type of activationD. A regularization techniqueAnswer: BExplanation: Attention computes weighted combinations of inputs based on relevance scores. citeturn0search4
28. Scaled dot-product attention is defined as:A. softmax((QKᵀ)/√dₖ) VB. softmax(QKᵀ) VC. relu((QKᵀ)/dₖ) VD. sigmoid(QKᵀ) VAnswer: AExplanation: Dividing by √dₖ prevents large dot‑product magnitudes that impair softmax gradients. citeturn0search4
29. Multi-head attention allows the model to:A. Attend to multiple positions in different waysB. Use multiple GPUsC. Perform convolution operationsD. Normalize activationsAnswer: AExplanation: Parallel attention heads capture diverse relationships in the same inputs. citeturn0search4
30. Positional encoding in transformers is necessary because:A. Transformers lack recurrence and convolutionB. It normalizes dataC. It reduces overfittingD. It speeds up computationAnswer: AExplanation: Without recurrence, positional encodings inject sequence order information. citeturn0search9
31. “Layer normalization” differs from batch normalization in that it:A. Normalizes across features instead of batchesB. Requires larger batch sizesC. Cannot be used in transformersD. Slows down trainingAnswer: AExplanation: Layer norm standardizes activations per sample, making it suitable for variable batch sizes. citeturn0search4
32. Which optimizer uses both momentum and adaptive learning rates?A. SGDB. AdagradC. AdamD. AdaDeltaAnswer: CExplanation: Adam combines momentum (first moment) with RMSProp‑style scaling (second moment). citeturn0search3
33. Learning rate scheduling can help by:A. Keeping learning rate constantB. Adjusting learning rate during trainingC. Removing the need for validationD. Regularizing weightsAnswer: BExplanation: Dynamically reducing or increasing the rate can improve convergence and final performance. citeturn0search3
34. Gradient descent variants like Adam improve upon SGD by:A. Using a fixed learning rateB. Tracking first and second moments of gradientsC. Reducing model sizeD. Increasing batch sizeAnswer: BExplanation: Moment estimates provide smoother and more efficient parameter updates. citeturn0search3
35. Which regularization technique adds a penalty proportional to the squared magnitude of weights?A. L1 regularizationB. L2 regularizationC. DropoutD. Batch normalizationAnswer: BExplanation: L2 regularization (weight decay) discourages large weights by adding their squared sum to the loss. citeturn0search10
36. Data augmentation for image data can include:A. RotationB. FlippingC. ScalingD. All of the aboveAnswer: DExplanation: Random transforms increase training diversity and reduce overfitting. citeturn0search6
37. Transfer learning typically involves:A. Training from scratch on a new datasetB. Fine-tuning a pre-trained model on new dataC. Using only the pre-trained model without changesD. Removing last layers permanentlyAnswer: BExplanation: Pre-trained weights provide good initialization, speeding convergence on related tasks. citeturn0search10
38. Which metric is suitable for imbalanced classification tasks?A. AccuracyB. F1-scoreC. MSED. MAEAnswer: BExplanation: F1‑score balances precision and recall, giving a better signal when classes are skewed. citeturn0search10
39. Confusion matrix elements include:A. True positives, false positivesB. Precision, recallC. Loss, accuracyD. Epochs, batchesAnswer: AExplanation: A confusion matrix tabulates counts of correct and incorrect predictions per class. citeturn0search10
40. Exploding gradients can be mitigated by:A. Gradient clippingB. Using deeper networksC. Increasing batch sizeD. Removing activation functionsAnswer: AExplanation: Clipping limits gradient magnitudes, preventing extremely large updates. citeturn0search10
41. Vanishing gradients are more problematic in:A. Shallow networksB. Deep networksC. Linear regressionD. Decision treesAnswer: BExplanation: Longer chains of derivatives amplify shrinking effects, hindering learning in very deep nets. citeturn0search10
42. Which model is best suited for sequence-to-sequence tasks?A. CNNB. TransformerC. Random ForestD. K-meansAnswer: BExplanation: Transformers with attention handle variable-length input/output efficiently without recurrence. citeturn0search4
43. Self-supervised learning can exploit:A. Labeled data onlyB. Unlabeled data with pretext tasksC. Reinforcement signalsD. None of the aboveAnswer: BExplanation: Models learn representations by solving tasks derived from unlabeled data itself. citeturn0search10
44. Contrastive learning aims to:A. Maximize similarity between different augmentations of the same sampleB. Generate synthetic dataC. Perform classificationD. Regularize modelsAnswer: AExplanation: Positive pairs are pulled together in embedding space, negatives pushed apart. citeturn0search10
45. Siamese networks are used for:A. Image classificationB. Similarity learningC. ClusteringD. RegressionAnswer: BExplanation: Twin networks share weights to embed inputs for distance‑based comparison. citeturn0search10
46. Which architecture leverages residual connections and batch norm heavily?A. VGGB. ResNetC. AlexNetD. LeNetAnswer: BExplanation: ResNet stacks identity‑mapping skip connections interspersed with batch norms. citeturn0search8
47. In object detection, “IoU” stands for:A. Intersection over UnionB. Input over OutputC. Integration of UnitsD. Instance over UpdateAnswer: AExplanation: IoU measures overlap between predicted and ground‑truth bounding boxes. citeturn0search10
48. Which loss is used for bounding-box regression?A. Cross-entropyB. Smooth L1C. HingeD. MSEAnswer: BExplanation: Smooth L1 (Huber) balances sensitivity to outliers with stable gradients. citeturn0search10
49. Semantic segmentation assigns:A. One class per imageB. One class per pixelC. Bounding boxes per objectD. Feature maps per layerAnswer: BExplanation: Each pixel is labeled with a class, producing dense per-pixel predictions. citeturn0search10
50. Instance segmentation combines:A. Classification and regressionB. Object detection and semantic segmentationC. Clustering and regressionD. None of the aboveAnswer: BExplanation: It detects objects (with boxes) and labels each pixel within each instance. citeturn0search10


1. Batch 2: Questions 51–100
2. Below is the second batch of 50 generative AI MCQs for a QA Tools and Test Development Intern role at NVIDIA. Each question is followed by the correct answer and a concise explanation, with citations to authoritative sources.

1. What is the key innovation introduced in the "Attention Is All You Need" paper?A. Recurrent connectionsB. Convolutional layersC. Self-attention mechanismD. Residual networksAnswer: CExplanation: The Transformer architecture replaces recurrence with a self‑attention mechanism, allowing each position in the input to attend directly to all other positions, improving parallelism and modeling long-range dependencies citeturn0search0.
2. Which of the following best describes the Transformer’s positional encoding?A. Learned embeddings for positionsB. Fixed sinusoidal functions added to input embeddingsC. Recurrent hidden statesD. Convolution over positionsAnswer: BExplanation: Transformers use fixed sinusoidal positional encodings added to token embeddings to inject sequence order information without recurrence citeturn0search0.
3. Why is the attention score scaled by √dₖ in scaled dot-product attention?A. To improve gradient flowB. To normalize the softmax input varianceC. To reduce computational complexityD. To incorporate positional biasAnswer: BExplanation: Dividing by √dₖ prevents the dot products from growing too large in magnitude, which stabilizes gradients when passed through the softmax citeturn0search0.
4. What advantage does multi-head attention offer over single-head attention?A. Faster inferenceB. Parameter reductionC. Modeling information from multiple representation subspacesD. Lower memory usageAnswer: CExplanation: Multi-head attention projects queries, keys, and values into multiple subspaces, enabling the model to jointly attend to information at different positions and representation subspaces citeturn0search0.
5. In Generative Pre-trained Transformers (GPT), what role do positional embeddings play?A. They replace token embeddingsB. They provide the model with information about the relative positions of tokensC. They are used only during fine‑tuningD. They enable cropping of input sequencesAnswer: BExplanation: Positional embeddings encode token position information so that the transformer can distinguish different orderings, crucial for language modeling citeturn0search0.
6. Which organization developed GPT-4?A. GoogleB. FacebookC. OpenAID. NVIDIAAnswer: CExplanation: GPT-4 was developed by OpenAI as the successor to GPT-3 in their series of large language models citeturn1search1.
7. When was GPT-4 officially released?A. June 2022B. January 2023C. March 2023D. September 2023Answer: CExplanation: GPT-4 was released on March 14, 2023, marking a significant step forward in generative AI capabilities citeturn1search0.
8. What is Retrieval-Augmented Generation (RAG)?A. A model compression techniqueB. A way to incorporate external knowledge into LLMs at inference timeC. A new attention mechanismD. A data augmentation methodAnswer: BExplanation: RAG augments LLMs by retrieving relevant documents from an external corpus or vector database, grounding outputs in up-to-date knowledge citeturn0search2.
9. Which of these is a generative modeling approach used for image synthesis?A. AutoencodersB. GANsC. Diffusion modelsD. RNNsAnswer: CExplanation: Diffusion models learn to generate images by reversing a gradual noising process, producing high-quality samples citeturn0search3.
10. How does Stable Diffusion generate images?A. By autoregressive pixel-by-pixel samplingB. By iterative denoising in latent spaceC. By GAN adversarial trainingD. By variational inferenceAnswer: BExplanation: Stable Diffusion operates in a lower-dimensional latent space and iteratively denoises noise vectors to produce images citeturn0search3.
11. Which architecture excels at modeling long-range dependencies in sequences?A. CNNB. RNNC. TransformerD. MLPAnswer: CExplanation: Transformers, via self‑attention, can directly attend to all positions in a sequence, capturing dependencies irrespective of distance citeturn0search0.
12. What is the primary purpose of beam search in language model decoding?A. To introduce randomnessB. To find the single most probable outputC. To approximate the top-k next tokensD. To explore multiple candidate sequences concurrentlyAnswer: DExplanation: Beam search maintains multiple hypotheses at each step, balancing exploration and exploitation to find high-probability sequences citeturn0search4.
13. In top‑k sampling, what does "k" refer to?A. Number of previous tokens consideredB. Size of the vocabularyC. Number of tokens with highest probabilities keptD. Temperature parameterAnswer: CExplanation: Top‑k sampling restricts the next-token choices to the k most probable tokens, promoting diversity while avoiding unlikely options citeturn0search5.
14. What is the nucleus (top‑p) sampling strategy?A. Sampling from the top k tokensB. Sampling tokens until cumulative probability ≥ pC. Sampling tokens based on temperatureD. Sampling tokens uniformlyAnswer: BExplanation: Nucleus sampling selects the smallest set of tokens whose cumulative probability is at least p, dynamically adjusting the candidate pool size citeturn0search6.
15. What effect does increasing the temperature have on language model sampling?A. Makes output more deterministicB. Sharpens the probability distributionC. Flattens the distribution, increasing randomnessD. Reduces vocabulary sizeAnswer: CExplanation: A higher temperature divides logits by a larger value, flattening the softmax distribution and increasing sampling randomness citeturn2search0.
16. What is the goal of prompt engineering?A. Training new parametersB. Crafting inputs to elicit desired model behaviorC. Reducing model sizeD. Changing model architectureAnswer: BExplanation: Prompt engineering designs and refines input prompts to steer LLMs toward specific types of responses without altering model weights citeturn0search7.
17. What distinguishes few‑shot prompting from zero‑shot prompting?A. Few‑shot uses explicit examples; zero‑shot uses only instructionsB. Few‑shot uses larger modelsC. Zero‑shot provides no context at allD. Zero‑shot fine‑tunes the modelAnswer: AExplanation: Few‑shot prompting includes a small number of input–output demonstrations in the prompt, while zero‑shot relies solely on instructions citeturn0search7.
18. What is chain‑of‑thought prompting?A. Prompt where the model lists its reasoning steps explicitlyB. Prompt that uses a chain of tokensC. A fine‑tuning techniqueD. A type of data augmentationAnswer: AExplanation: Chain‑of‑thought prompts ask the model to produce intermediate reasoning steps, improving performance on complex tasks citeturn0search8.
19. Which technique combines retrieval and generation in LLMs?A. LoRAB. Adapter tuningC. RAGD. Prompt tuningAnswer: CExplanation: Retrieval‑Augmented Generation enhances the LLM’s responses by retrieving relevant documents to ground its generated outputs citeturn0search2.
20. What is knowledge distillation?A. Reducing model precisionB. Transferring knowledge from a large teacher model to a smaller student modelC. Data augmentationD. Prompt engineeringAnswer: BExplanation: Knowledge distillation trains a smaller “student” model to mimic a larger “teacher” model’s outputs, achieving efficiency with minimal accuracy loss citeturn3search11.
21. Which open-source library provides transformer models for NLP?A. PyTorch LightningB. TensorFlow HubC. Hugging Face TransformersD. spaCyAnswer: CExplanation: Hugging Face Transformers offers a wide range of pre‑trained models and tokenizers for deep learning NLP tasks citeturn0search11.
22. What does LoRA stand for?A. Low-Rank AdaptationB. Language-Only Relevance AlignmentC. Latent Optimized Resampling AlgorithmD. Learning-Oriented Reinforcement AdaptationAnswer: AExplanation: LoRA (Low‑Rank Adaptation) injects low‑rank weight updates into transformer layers for parameter‑efficient fine‑tuning citeturn0search9.
23. How does LoRA reduce fine‑tuning parameters?A. By freezing all weightsB. By decomposing updates into low‑rank matrices added to frozen weightsC. By quantizing activationsD. By knowledge distillationAnswer: BExplanation: LoRA freezes original weights and learns low‑rank matrices that approximate full weight updates, significantly cutting trainable parameters citeturn0search9.
24. What are adapter modules?A. Small networks inserted between transformer blocks and fine‑tunedB. Extra positional encodingsC. Data preprocessing stepsD. A type of quantizationAnswer: AExplanation: Adapters add trainable layers within transformer blocks while freezing the main parameters, enabling parameter‑efficient tuning citeturn4search0.
25. Which approach fine‑tunes only a small set of new parameters for downstream tasks?A. Full fine‑tuningB. Parameter-efficient fine‑tuning (PEFT)C. Model pruningD. Data augmentationAnswer: BExplanation: PEFT methods like adapters and LoRA update minimal parameters, preserving most of the pre-trained model for efficiency citeturn4search9.
26. What is the purpose of model quantization?A. Increase model sizeB. Reduce numeric precision of weights and activations for efficiencyC. Add more layersD. Improve accuracyAnswer: BExplanation: Quantization converts weights/activations to lower precision (e.g., INT8), reducing memory footprint and speeding up inference citeturn0search10.
27. How can knowledge graphs enhance generative AI?A. By compressing embeddingsB. By providing structured factual context to reduce hallucinationsC. By replacing attention mechanismsD. By training data augmentationAnswer: BExplanation: Knowledge graphs ground generative models with structured facts and relationships, improving accuracy and consistency citeturn5search0.
28. What are embeddings in the context of LLMs?A. One-hot vectorsB. Sparse representationsC. Dense continuous vectors capturing semanticsD. Token indicesAnswer: CExplanation: Embeddings map discrete tokens to dense vectors in continuous space, capturing semantic relationships for downstream tasks citeturn6search0.
29. Semantic search relies primarily on what underlying technique?A. Exact string matchingB. Vector similarity search over embeddingsC. Rule-based filteringD. Beam searchAnswer: BExplanation: Semantic search computes similarity between query and document embeddings (e.g., cosine similarity) to retrieve relevant results citeturn6search0.
30. In RAG, which storage system is typically used for retrieval?A. Relational databaseB. Key‑value storeC. Vector databaseD. File systemAnswer: CExplanation: RAG often leverages vector databases to store and query high-dimensional embeddings for efficient similarity search citeturn7search0.
31. Diffusion models generate samples by learning to reverse what process?A. ConvolutionB. Recurrent unfoldingC. Gradual noise injectionD. PCA decompositionAnswer: CExplanation: Diffusion models learn to undo a forward process that gradually adds Gaussian noise to data citeturn0search3.
32. What does DDPM stand for?A. Discrete Deep Parameter ModelB. Denoising Diffusion Probabilistic ModelC. Deep Deterministic Policy ModelD. Diffusion‑Driven Predictive ModelAnswer: BExplanation: DDPM refers to Denoising Diffusion Probabilistic Models, which improve diffusion models via variational inference citeturn8search0.
33. Stable Diffusion operates in what space?A. Pixel spaceB. Latent spaceC. Vocabulary spaceD. Feature spaceAnswer: BExplanation: Stable Diffusion is a latent diffusion model, applying diffusion processes in a compressed latent representation rather than raw pixels citeturn8search1.
34. What is image inpainting?A. Adding noise to imagesB. Restoring missing or corrupted regions of an imageC. Classifying image segmentsD. Enhancing image resolutionAnswer: BExplanation: Inpainting fills masked or damaged areas by inferring content from surrounding pixels or generative priors citeturn9search1.
35. Which failure mode is GAN training particularly prone to?A. OverfittingB. UnderflowC. Mode collapseD. Vanishing activationAnswer: CExplanation: Mode collapse occurs when a GAN’s generator produces limited diversity, focusing on a few modes of the data distribution citeturn10search0.
36. GAN training involves adversarial loss between which two networks?A. Encoder and decoderB. Teacher and studentC. Generator and discriminatorD. Actor and criticAnswer: CExplanation: A GAN’s generator and discriminator compete via adversarial loss, with the generator aiming to fool the discriminator citeturn10search1.
37. What is the primary metric optimized by Wasserstein GAN (WGAN)?A. Jensen–Shannon divergenceB. Earth Mover’s (Wasserstein) distanceC. KL divergenceD. Cosine similarityAnswer: BExplanation: WGAN uses the Earth Mover’s distance (Wasserstein-1) to provide a more stable training signal and mitigate mode collapse citeturn10search2.
38. What does FID measure in generative image models?A. Pixel-wise differenceB. Inception activations’ distribution distanceC. BLEU score on captionsD. Perplexity of generated textAnswer: BExplanation: Fréchet Inception Distance compares mean/covariance of Inception embeddings between generated and real images to assess quality citeturn11search0.
39. Which metric evaluates machine translation quality by comparing n‑grams?A. ROUGEB. BLEUC. WERD. FIDAnswer: BExplanation: BLEU (Bilingual Evaluation Understudy) scores candidate translations against reference texts based on modified n‑gram precision citeturn12search0.
40. What does perplexity measure in language models?A. Vocabulary sizeB. Model uncertainty or average branching factorC. Inference speedD. Sentence lengthAnswer: BExplanation: Perplexity quantifies how well a probability model predicts a test set; lower perplexity implies better predictive power citeturn13search1.
41. Why is nucleus sampling considered more flexible than top‑k sampling?A. It fixes the token countB. It adapts the candidate pool size based on cumulative probabilityC. It uses temperatureD. It reduces model sizeAnswer: BExplanation: Nucleus sampling dynamically adjusts the number of candidates to include only those that jointly exceed a probability threshold, balancing diversity and coherence citeturn0search6.
42. Which prompting method explicitly asks the model to enumerate its reasoning steps?A. Zero‑shotB. Few‑shotC. Chain‑of‑thoughtD. Prompt tuningAnswer: CExplanation: Chain‑of‑thought prompting guides LLMs to show intermediate reasoning, improving performance on multi‑step tasks citeturn0search8.
43. What is prompt tuning?A. Freezing an LLM and learning soft prompt embeddingsB. Adding more layers to a modelC. Quantizing model weightsD. Knowledge distillationAnswer: AExplanation: Prompt tuning optimizes a small set of continuous “soft” prompt embeddings prepended to inputs while keeping model weights frozen citeturn14search0.
44. LoRA is an example of which fine‑tuning strategy?A. Full fine‑tuningB. Prompt tuningC. Low‑rank adaptationD. Static quantizationAnswer: CExplanation: LoRA implements low‑rank adaptation by injecting learnable low‑rank matrices into transformer weights citeturn0search9.
45. What is the main benefit of LoRA?A. Improves model accuracy drasticallyB. Reduces number of trainable parameters during fine‑tuningC. Enables dynamic quantizationD. Introduces new attention headsAnswer: BExplanation: LoRA provides parameter‑efficient fine‑tuning by learning low‑rank updates instead of full weight matrices citeturn0search9.
46. Which quantization approach adapts scale factors at runtime?A. Static quantizationB. Dynamic quantizationC. Post‑training static quantizationD. QAT (quantization-aware training)Answer: BExplanation: Dynamic quantization quantizes weights ahead of time and determines activation scales on the fly during inference citeturn15search0.
47. Static quantization differs from dynamic quantization in that static quantization requires what step?A. No calibrationB. One‑time calibration with representative dataC. Prompt tuningD. Adapter insertionAnswer: BExplanation: Static quantization calibrates both weights and activations with a representative dataset before inference, fixing quantization parameters citeturn15search2.
48. Which model generates images from textual prompts using a discrete VAE and autoregressive transformer?A. Stable DiffusionB. DALL·EC. CLIPD. VQ-VAEAnswer: BExplanation: DALL·E uses a discrete VAE to tokenize images and an autoregressive transformer to generate image tokens from text prompts citeturn0search13.
49. What does CLIP stand for?A. Contrastive Language-Image Pre-trainingB. Conditional Language-Image PredictionC. Cross-Modal Language-Image ProcessingD. Clustering via Image-PrologAnswer: AExplanation: CLIP (Contrastive Language-Image Pre‑training) learns joint embeddings of text and images via a contrastive objective citeturn16search0.
50. Approximately how many image–text pairs was CLIP trained on?A. 10 millionB. 100 millionC. 400 millionD. 1 billionAnswer: C
51. Batch 3: Questions 101–150 (AI Tools)
52. Which library provides GPU‑accelerated primitives for deep neural networks on NVIDIA GPUs?A. cuBLASB. cuDNNC. NCCLD. TensorRTAnswer: BExplanation: cuDNN offers highly tuned implementations of core routines (convolution, pooling, activation) for deep learning on NVIDIA GPUs citeturn1search0.
53. What is the primary purpose of TensorRT?A. Training large modelsB. High‑performance inference optimizationC. Data preprocessingD. Model visualizationAnswer: BExplanation: TensorRT is an SDK designed to optimize and accelerate inference of deep learning models on NVIDIA hardware citeturn1search1.
54. ONNX is best described as:A. A deep learning frameworkB. An open format for model interoperabilityC. A GPU schedulerD. A data visualization libraryAnswer: BExplanation: ONNX defines a standard IR and operator set to allow models to be transferred between frameworks citeturn1search2.
55. MLflow’s core functionality includes:A. GPU kernel profilingB. Experiment tracking and model registryC. Distributed training orchestrationD. Real‑time inference servingAnswer: BExplanation: MLflow provides APIs and UI for logging runs, parameters, metrics, and versioned model storage citeturn1search3.
56. Kubeflow Pipelines are used for:A. CI/CD of microservicesB. Defining and running portable ML workflows on KubernetesC. Real‑time data streamingD. Distributed hyperparameter search onlyAnswer: BExplanation: Kubeflow Pipelines let you author, orchestrate, and deploy ML workflows as containerized components on Kubernetes citeturn1search4.
57. Which tool accelerates all‑reduce and other collectives for multi‑GPU training?A. cuBLASB. NCCLC. cuDNND. TensorRTAnswer: BExplanation: NCCL (NVIDIA Collective Communications Library) implements optimized multi‑GPU and multi‑node collectives like all‑reduce citeturn1search0.
58. What role does TensorFlow Serving play?A. Model trainingB. Batch data preprocessingC. Production‑grade model servingD. Hardware monitoringAnswer: CExplanation: TensorFlow Serving provides a flexible, high‑performance serving system for machine learning models in production.
59. Which framework is tailored for real‑time video analytics on NVIDIA GPUs?A. DeepStreamB. TensorRTC. RAPIDSD. TritonAnswer: AExplanation: NVIDIA DeepStream SDK streamlines building GPU‑accelerated video analytics and AI pipelines.
60. What does RAPIDS provide?A. Data visualization toolsB. GPU‑accelerated data science librariesC. Model interpretabilityD. Distributed batch processingAnswer: BExplanation: RAPIDS offers GPU‑accelerated equivalents of pandas, scikit‑learn, and other data‑science libraries.
61. LangChain is a framework primarily for:A. Computer vision preprocessingB. LLM application development with chaining prompts, memory, and retrievalC. GPU kernel optimizationD. Time series forecastingAnswer: BExplanation: LangChain provides abstractions for prompt chaining, memory management, and retrieval in LLM apps.
62. FAISS is used for:A. Fast similarity search of vector embeddingsB. Distributed model trainingC. Hyperparameter tuningD. Model serializationAnswer: AExplanation: Facebook AI Similarity Search (FAISS) enables efficient nearest‑neighbor search in high‑dimensional spaces.
63. Weights & Biases (W&B) primarily offers:A. Container orchestrationB. Experiment tracking and visualizationC. GPU profilingD. Model compressionAnswer: BExplanation: W&B provides dashboards for tracking experiments, comparing runs, and sharing results.
64. DVC (Data Version Control) is designed for:A. Code versioningB. Data and model versioning in ML projectsC. CI/CD pipelinesD. Container registryAnswer: BExplanation: DVC tracks data files, model binaries, and ML pipelines alongside Git.
65. Seldon Core facilitates:A. Data labelingB. Scalable model deployment on KubernetesC. Model training pipelinesD. Real‑time data ingestionAnswer: BExplanation: Seldon Core packages ML models as microservices for scalable deployment on Kubernetes.
66. MLPerf benchmarks are used to measure:A. Web server throughputB. ML training and inference performance across hardware and softwareC. Database query latencyD. GPU memory usageAnswer: BExplanation: MLPerf provides standardized suites for evaluating ML system performance.
67. Apex (NVIDIA) provides:A. Data annotation toolsB. Automated mixed‑precision training in PyTorchC. Hyperparameter tuningD. Model explainabilityAnswer: BExplanation: NVIDIA Apex enables AMP (Automatic Mixed Precision) to speed up training and reduce memory.
68. NVIDIA Triton Inference Server supports:A. CPU only inferenceB. GPU and CPU inference for multiple frameworksC. Model training orchestrationD. Batch data preprocessingAnswer: BExplanation: Triton serves models from TensorFlow, PyTorch, ONNX, and custom backends on GPU/CPU.
69. Which toolchain enables efficient inference on large language models with Triton and NeMo?A. TensorFlow ServingB. TensorRT‑LLMC. HorovodD. DeepStreamAnswer: BExplanation: TensorRT‑LLM integrates with NVIDIA NeMo to optimize LLM inference on GPUs citeturn1search1.
70. Which open‑source project helps orchestrate ML workflows on Kubernetes?A. AirflowB. KubeflowC. PrefectD. ArgoAnswer: BExplanation: Kubeflow natively runs on Kubernetes, providing pipelines, notebooks, and hyperparameter tuning.
71. OpenVINO is Intel’s toolkit for:A. GPU accelerationB. Optimizing inference on Intel hardwareC. Data version controlD. Container orchestrationAnswer: BExplanation: OpenVINO accelerates deep learning inference on Intel CPUs, GPUs, and VPUs.
72. Ray is a framework for:A. Distributed Python computing and reinforcement learningB. Web developmentC. Model interpretabilityD. Container managementAnswer: AExplanation: Ray provides APIs for distributed training, hyperparameter tuning, and serving.
73. Horovod improves:A. Single‑GPU training speedB. Distributed deep learning training scalabilityC. Real‑time inferenceD. Model compressionAnswer: BExplanation: Horovod uses efficient all‑reduce algorithms to scale training across GPUs/nodes.
74. Detectron2 is a library for:A. Language modelingB. Computer vision research (object detection/segmentation)C. Time series forecastingD. Text summarizationAnswer: BExplanation: Detectron2, by Facebook AI Research, provides state‑of‑the‑art detection and segmentation models.
75. YOLO stands for:A. You Only Learn OnceB. You Only Look OnceC. Your Object Localization OptimizerD. None of the aboveAnswer: BExplanation: YOLO is a real‑time object detection system that predicts bounding boxes and classes in one pass.
76. OpenAI Codex is designed for:A. Speech recognitionB. Code generation from natural languageC. Image captioningD. Graph analyticsAnswer: BExplanation: Codex powers tools like GitHub Copilot, translating NL prompts into code.
77. SHAP is a library for:A. Data versioningB. Model interpretability via SHapley Additive exPlanationsC. GPU profilingD. Distributed trainingAnswer: BExplanation: SHAP computes feature attributions based on game‑theoretic Shapley values.
78. Evidently AI helps with:A. Data labelingB. Model performance monitoring and drift detectionC. Model training orchestrationD. Container managementAnswer: BExplanation: Evidently provides dashboards to monitor data/model drift in production.
79. LMOps refers to:A. Large Model OperationsB. Lightweight Model OperationsC. Language Model OpsD. None of the aboveAnswer: CExplanation: LMOps focuses on operationalizing large language models with best practices.
80. NVIDIA Merlin is a framework for:A. Time series forecastingB. Recommender systems and feature engineeringC. Computer vision onlyD. Speech recognitionAnswer: BExplanation: Merlin accelerates end‑to‑end recommender workflows on GPUs.
81. ClipCap combines CLIP embeddings with a transformer for:A. Video summarizationB. Image captioningC. Text classificationD. Speech synthesisAnswer: BExplanation: ClipCap uses CLIP’s image embeddings and a language model decoder to generate captions.
82. LMFlow focuses on:A. Model compressionB. Fine‑tuning and serving large language modelsC. Distributed trainingD. Data preprocessingAnswer: BExplanation: LMFlow provides tools for efficient LLM fine‑tuning, quantization, and deployment.
83. Which library offers a unified API for quantization, pruning, and distillation?A. ONNXB. TensorRT Model Optimizer (nvidia-modelopt)C. RAPIDSD. HorovodAnswer: BExplanation: NVIDIA’s Model Optimizer library (nvidia-modelopt) bundles various model compression techniques citeturn1search11.
84. Which MLFlow component manages model stages and versions?A. Tracking APIB. Model RegistryC. ProjectsD. MetricsAnswer: BExplanation: The Model Registry provides a collaborative store for versioning and stage transitions citeturn1search13.
85. Which component in Kubeflow handles hyperparameter tuning?A. KatibB. PipelinesC. NotebooksD. ServingAnswer: AExplanation: Katib provides automated hyperparameter tuning with Bayesian optimization, grid/random search.
86. Which tool is used for automated mixed‑precision training in PyTorch?A. ApexB. KerasC. HorovodD. DVCAnswer: AExplanation: NVIDIA Apex’s AMP automatically applies mixed precision for speed and memory benefits.
87. ONNX Runtime is primarily used for:A. Model trainingB. Cross‑platform inferenceC. Data labelingD. GPU kernel profilingAnswer: BExplanation: ONNX Runtime executes ONNX models efficiently across CPU, GPU, and other accelerators.
88. Which NVIDIA tool profiles end‑to‑end GPU workloads (CUDA, kernels, CPU)?A. nvprofB. Nsight SystemsC. cuda-memcheckD. cProfileAnswer: BExplanation: Nsight Systems provides timeline tracing of CPU+GPU interactions for performance analysis citeturn0search10.
89. cuda-memcheck is used for:A. Performance profilingB. Detecting memory errors in CUDA applicationsC. Debugging Python codeD. Logging metricsAnswer: BExplanation: cuda-memcheck finds out‑of‑bounds, misaligned accesses, and race conditions in GPU code citeturn0search11.
90. Testcontainers helps with:A. Unit testingB. Integration testing with real containers (e.g., databases, Kafka) in JVM/PythonC. Static code analysisD. Load testingAnswer: BExplanation: Testcontainers spins up Docker containers for reliable, reproducible integration tests.
91. Which MLPerf suite measures inference latency and throughput?A. Training benchmarkB. Inference benchmarkC. Profiling benchmarkD. Operator benchmarkAnswer: BExplanation: MLPerf Inference evaluates model serving performance (latency, throughput) on various hardware citeturn1news59.

1. Batch 4: Questions 151–200 (QA Tools & Test Development)
2. In pytest, a “fixture” is defined by decorating a function with:A. @pytest.setupB. @pytest.fixtureC. @pytest.testD. @pytest.parametrizeAnswer: BExplanation: @pytest.fixture marks a function as a fixture, providing setup/teardown or test data citeturn0search0.
3. @pytest.mark.parametrize is used to:A. Skip testsB. Generate multiple test cases with different inputsC. Define fixturesD. Benchmark testsAnswer: BExplanation: It allows a test function to run with multiple argument sets citeturn0search1.
4. Selenium WebDriver is standardized by which organization?A. W3CB. ISOC. IEEED. ECMAAnswer: AExplanation: Selenium WebDriver is a W3C Recommendation for browser automation citeturn0search2.
5. In Jenkins, pipelines are typically defined in:A. DockerfileB. Jenkinsfile (Groovy)C. MakefileD. .gitlab-ci.ymlAnswer: BExplanation: A Jenkinsfile (written in Groovy or Declarative syntax) defines CI/CD stages and steps citeturn0search3.
6. Behave is a BDD framework for Python that uses:A. JSON test definitionsB. Gherkin syntaxC. YAML specificationsD. XML test casesAnswer: BExplanation: Behave reads scenarios in Gherkin (Given/When/Then) and maps steps to Python code citeturn0search4.
7. Robot Framework’s SeleniumLibrary provides:A. REST API testingB. Web testing keywords powered by Selenium WebDriverC. Mobile app automationD. Performance profilingAnswer: BExplanation: SeleniumLibrary supplies keyword-driven web automation for Robot Framework citeturn0search5.
8. In JMeter, a Thread Group controls:A. Number of virtual users (threads) and test durationB. Database connectionsC. API endpointsD. Assertion logicAnswer: AExplanation: Thread Groups define user load and scheduling in a JMeter Test Plan citeturn0search6.
9. Contract testing for microservices, ensuring compatibility, is commonly implemented with:A. JMeterB. PactC. SeleniumD. cProfileAnswer: BExplanation: Pact defines and verifies contracts between service providers and consumers citeturn0search7.
10. Chaos Engineering, simulating failures in production, is popularized by which tool?A. GremlinB. JMeterC. PostmanD. cProfileAnswer: AExplanation: Gremlin lets teams inject faults (CPU/memory/network) to test system resilience citeturn0search8.
11. Python’s built‑in cProfile module is used for:A. Unit testingB. Deterministic performance profilingC. Memory leak detectionD. Code coverageAnswer: BExplanation: cProfile tracks function call counts and timing for performance analysis citeturn0search9.
12. Nsight Systems is a tool for:A. GPU memory leak detectionB. End‑to‑end GPU+CPU performance tracingC. Distributed training orchestrationD. API testingAnswer: BExplanation: Nsight Systems captures timelines of CPU threads, CUDA kernels, and OS events for optimization citeturn0search10.
13. Which tool detects out‑of‑bounds and misaligned memory accesses in CUDA kernels?A. cuda-memcheckB. nvprofC. cProfileD. gdbAnswer: AExplanation: cuda-memcheck identifies memory errors and race conditions in GPU code citeturn0search11.
14. Infrastructure as Code (IaC) tools include:A. Terraform and AnsibleB. JMeter and SeleniumC. Behave and CucumberD. cProfile and NsightAnswer: AExplanation: Terraform provisions cloud resources declaratively; Ansible configures infrastructure via playbooks citeturn0search13.
15. Test environment provisioning can be automated with:A. Packer, Terraform, and AnsibleB. pytest, behave, and RobotC. JMeter, Locust, and GatlingD. Docker Compose onlyAnswer: AExplanation: Packer builds machine images; Terraform and Ansible provision and configure environments citeturn0search13.
16. GitLab CI/CD pipelines are defined in:A. .gitlab-ci.ymlB. JenkinsfileC. azure-pipelines.ymlD. travis.ymlAnswer: AExplanation: GitLab uses .gitlab-ci.yml at the project root to define jobs and stages citeturn0search14.
17. Shift‑left testing encourages:A. Testing only in productionB. Early involvement of QA and automation in the development cycleC. Manual testing after deploymentD. Performance testing onlyAnswer: BExplanation: Shift‑left moves testing earlier to catch defects sooner and reduce costs.
18. SDET stands for:A. Software Development Engineer in TestB. Systems Deployment Engineer in TrainingC. Security Development Engineer in TestD. Software Design Engineer for TestingAnswer: AExplanation: SDETs are developers who focus on designing and writing test frameworks and automation.
19. Code coverage metrics measure:A. Number of tests executedB. Proportion of code lines exercised by testsC. Performance of unit testsD. Memory usage during testsAnswer: BExplanation: Coverage tools report which lines or branches have been tested.
20. pytest‑cov is a plugin for:A. Profiling testsB. Measuring test coverage in pytestC. Parallel test executionD. Database testingAnswer: BExplanation: pytest‑cov integrates coverage.py to generate coverage reports.
21. Allure is used for:A. Test result visualization and reportingB. Load testingC. API mockingD. Security scanningAnswer: AExplanation: Allure generates interactive HTML reports from test results.
22. In Cucumber, scenarios are written in:A. JSONB. YAMLC. GherkinD. XMLAnswer: CExplanation: Gherkin uses plain‑language Given/When/Then syntax for BDD scenarios.
23. Page Object Model (POM) in Selenium promotes:A. Direct element locators in testsB. Encapsulating page structure and actions in classesC. Data‑driven testing onlyD. Performance profilingAnswer: BExplanation: POM organizes locators and actions per page to improve maintainability.
24. Implicit waits in Selenium:A. Poll for element presence up to a timeoutB. Pause execution unconditionallyC. Wait only for page loadD. Use JavaScript to waitAnswer: AExplanation: Implicit waits set a default polling timeout when locating elements.
25. Explicit waits in Selenium use:A. time.sleep()B. driver.implicitly_wait()C. WebDriverWait with expected conditionsD. None of the aboveAnswer: CExplanation: Explicit waits wait for specific conditions (e.g., element clickable) before proceeding.
26. Load testing tools include JMeter, Locust, and:A. Robot FrameworkB. GatlingC. BehaveD. cProfileAnswer: BExplanation: Gatling, written in Scala, simulates high loads with expressive DSLs.
27. Contract testing ensures:A. UI consistencyB. API compatibility between servicesC. Performance benchmarksD. Security complianceAnswer: BExplanation: Contract tests verify that provider and consumer agree on request/response schemas citeturn0search7.
28. Git hooks can automate:A. Pre‑commit linting and testsB. Database migrationsC. Docker builds onlyD. API testingAnswer: AExplanation: Hooks like pre‑commit or pre‑push can run linters and test suites before commits.
29. Static code analysis tools include:A. pylint, flake8, and ESLintB. JMeter and LocustC. Selenium and CypressD. pytest and unittestAnswer: AExplanation: Linters and static analyzers detect syntax/style issues without execution.
30. Security testing tools include:A. OWASP ZAP, Burp Suite, NessusB. JMeter, Gatling, LocustC. pytest, behave, RobotD. cProfile, nsightAnswer: AExplanation: These tools scan for vulnerabilities in web applications and APIs.
31. API testing tools include:A. Postman, REST Assured, SoapUIB. Selenium, Cypress, PlaywrightC. JMeter, Gatling, LocustD. cProfile, nsightAnswer: AExplanation: Postman offers GUI, REST Assured and SoapUI provide code‑based API tests.
32. Appium is used for:A. Web testingB. Mobile app automation on iOS and AndroidC. Desktop application testingD. Database testingAnswer: BExplanation: Appium drives native/hybrid apps via WebDriver protocol.
33. Performance profiling in Python can use:A. cProfile, line_profiler, memory_profilerB. pytest, behave, RobotC. Selenium, Cypress, PlaywrightD. JMeter, Gatling, LocustAnswer: AExplanation: Profilers measure CPU time (cProfile), line‑level time (line_profiler), and memory usage.
34. Chaos Monkey is part of:A. GremlinB. Netflix Simian ArmyC. JMeterD. PostmanAnswer: BExplanation: Chaos Monkey randomly terminates instances in production to test resilience.
35. Puppet and Chef are primarily used for:A. Test automationB. Configuration managementC. API testingD. Load testingAnswer: BExplanation: They automate provisioning and configuration of servers.
36. Which metric indicates how many statements are executed at least once by tests?A. Line coverageB. Branch coverageC. Mutation scoreD. Performance ratioAnswer: AExplanation: Line coverage reports which lines are executed during testing.
37. Mutation testing tools (e.g., MutPy) help measure:A. Code styleB. Test suite effectiveness by injecting code changesC. Performance regressionD. Memory leaksAnswer: BExplanation: Mutants are altered versions of code; surviving mutants indicate holes in tests.
38. SeleniumGrid is used to:A. Parallelize browser tests across multiple nodesB. Profile GPU testsC. Automate API testsD. Generate test dataAnswer: AExplanation: Grid distributes WebDriver sessions across browsers, versions, and platforms.
39. Which of the following is a behavior‑driven test runner for .NET?A. SpecFlowB. CucumberC. BehaveD. RobotAnswer: AExplanation: SpecFlow brings Gherkin BDD to the .NET ecosystem.
40. Azure Pipelines define workflows in:A. azure-pipelines.ymlB. .gitlab-ci.ymlC. JenkinsfileD. travis.ymlAnswer: AExplanation: Azure DevOps uses azure-pipelines.yml to configure CI/CD pipelines.
41. CircleCI workflows are configured in:A. .circleci/config.ymlB. JenkinsfileC. .gitlab-ci.ymlD. azure-pipelines.ymlAnswer: AExplanation: CircleCI reads .circleci/config.yml from the repository root.
42. Shift‑right testing focuses on:A. Early unit testsB. Testing in production and real‑user monitoringC. Integration tests onlyD. Static analysisAnswer: BExplanation: Shift‑right emphasizes validation under real‑world conditions post‑deployment.
43. Service virtualization tools (e.g., WireMock) allow:A. Simulating dependent APIs for testingB. Profiling servicesC. Load testing onlyD. Security scanningAnswer: AExplanation: WireMock and similar tools stub APIs so tests can run without real dependencies.
44. Which plugin integrates pytest with Jenkins for test reporting?A. pytest‑junitxmlB. pytest‑covC. pytest‑seleniumD. pytest‑runnerAnswer: AExplanation: pytest can output JUnit‑style XML for Jenkins to consume and display.
45. SeleniumBase is built on top of:A. Selenium WebDriver and pytest/behave/noseB. Cypress onlyC. Playwright onlyD. JMeter onlyAnswer: AExplanation: SeleniumBase extends WebDriver with fixtures, assertions, and page objects citeturn0search0.
46. Robot Framework’s test cases are organized in:A. .robot files with sections (Settings, Variables, Test Cases)B. JSON filesC. XML filesD. Python scripts onlyAnswer: AExplanation: .robot files use tabular syntax and keyword-driven approach.
47. Load testing scenario with varying user load is best modeled by:A. JMeter Thread Group with ramp‑up settingsB. pytest parametrizationC. Postman collectionsD. Robot Framework loopsAnswer: AExplanation: JMeter’s ramp-up controls how threads start over time, simulating ramping load.
48. Which tool can run Kubernetes operators locally in tests?A. Testcontainers with K3s module (Gefyra)B. Docker Compose onlyC. Minikube onlyD. Kind onlyAnswer: AExplanation: Testcontainers’ K3s integration (via Gefyra) spins up a lightweight Kubernetes cluster for operator testing citeturn0search12.
49. GitHub Actions workflows are defined in:A. .github/workflows/*.ymlB. .gitlab-ci.ymlC. JenkinsfileD. circleci.ymlAnswer: AExplanation: Actions reads YAML files in .github/workflows to configure jobs and triggers.
50. Continuous Delivery (CD) extends CI by:A. Automating merges onlyB. Automating deployment to staging/productionC. Writing tests automaticallyD. Profiling code coverageAnswer: BExplanation: CD pipelines automatically deploy successful builds to target environments.
51. Quality gates in CI/CD block:A. Builds that fail unit tests, linting, or coverage thresholdsB. Pull requests onlyC. Documentation updatesD. API deploymentsAnswer: AExplanation: Gates enforce standards (test success, code quality, security scans) before progressing.



"""

# PDF generation
pdf = FPDF()
pdf.set_auto_page_break(auto=True, margin=15)
pdf.add_page()
pdf.set_font("Arial", size=10)

# Split into individual questions (simulate block-wise parsing)
questions = html_snippet.strip().split("\n\n")
for question in questions:
    pdf.multi_cell(0, 10, question + "\n", border=0)

# Save the generated PDF
pdf_path = "/"
pdf.output(pdf_path)

pdf_path
